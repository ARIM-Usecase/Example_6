








!pip install pyod
!git clone https://github.com/ARIM-Usecase/Example_6.git
%cd Example_6





#I/Oライブラリ
import scipy.io as scio

# 汎用ライブラリ
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 機械学習
from sklearn.neighbors import LocalOutlierFactor as LOF
from sklearn import metrics
from pyod.models.lof import LOF as PYOD_LOF
from sklearn.svm import OneClassSVM

# 深層学習
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torchvision.transforms import Resize
from torch.utils.data import DataLoader, Dataset





# Data Preprocessing
def load_and_preprocess_data(filepath, scale_factor=6000):
    yxqdata = scio.loadmat(filepath)
    keys = ['X_training_normal', 'X_training', 'X_test', 'X_pre']
    data = {key: yxqdata[key] / scale_factor for key in keys}
    
    for key in data:
        data[key][data[key] < 0] = 0

    labels = {key: (yxqdata[key.replace('X', 'Y')]).squeeze() for key in keys}
    
    return data, labels


data, labels = load_and_preprocess_data('./data/CAO.mat')








def data_resize(data, a, b):
    data_1 = np.moveaxis(data, -1, 0)
    data_1 = torch.from_numpy(data_1.astype(np.float32)).unsqueeze(0)
    torch_resize = Resize([a, b])
    data_resize = torch_resize(data_1)
    data_resize = data_resize.squeeze(0).numpy()
    
    return np.moveaxis(data_resize, 0, -1)


for key in data:
    data[key] = data_resize(data[key], 80, 120)





# Dataset Definition
class yxqDataset(Dataset):
    def __init__(self, data, labels, transform=None):
        self.labels = labels
        self.imgs = data
        self.transform = transform

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        image = self.imgs[:, :, idx]
        label = self.labels[idx]
        if self.transform:
            image = self.transform(image)
        return image, label








data_tf = transforms.ToTensor()








def create_dataloader(data, labels, batch_size=16, shuffle=True):
    dataset = yxqDataset(data, labels, transform=data_tf)
    
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)


train_normal_loader = create_dataloader(data['X_training_normal'], labels['X_training_normal'])





# Define CAE
class ConvAutoencoder(nn.Module):
    def __init__(self):
        super(ConvAutoencoder, self).__init__()
        self.encoder1 = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),
            nn.ReLU()
        )
        self.encoder1_pool = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)
        self.encoder2 = nn.Sequential(
            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=0),
            nn.ReLU()
        )
        self.encoder2_pool = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)
        self.fc1 = nn.Sequential(
            nn.Flatten(),
            nn.Linear(32 * 19 * 29, 120),
            nn.Linear(120, 32)
        )
        self.fc2 = nn.Sequential(
            nn.Linear(32, 120),
            nn.Linear(120, 32 * 19 * 29),
            nn.Unflatten(1, (32, 19, 29))
        )
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=1, padding=0),
            nn.ReLU(),
            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=1, padding=1),
            nn.Tanh()
        )
        self.unpool1 = nn.MaxUnpool2d(kernel_size=2, stride=2)
        self.unpool2 = nn.MaxUnpool2d(kernel_size=2, stride=2)

    def forward(self, x):
        x = self.encoder1(x)
        x, ind1 = self.encoder1_pool(x)
        x = self.encoder2(x)
        x, ind2 = self.encoder2_pool(x)
        encoded = self.fc1(x)
        x = self.fc2(encoded)
        x = self.unpool1(x, ind2)
        x = self.decoder[0:2](x)
        x = self.unpool2(x, ind1)
        decoded = self.decoder[2:](x)
        
        return encoded, decoded








model = ConvAutoencoder()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)





#評価値の記録用データフレームの作成
df = pd.DataFrame(columns=['epoch', 'train_loss'])
df.to_csv('./result/CAO/loss/CAO_loss1.csv', index=False)





# Training Function
def train_model(num_epochs, model, train_loader, criterion, optimizer):
    train_losses = []
    for epoch in range(num_epochs):
        train_loss = 0.0
        model.train()
        for img, _ in train_loader:
            img = img.float()
            train_recon, train_recon2 = model(img)
            loss = criterion(train_recon2, img)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item() * img.size(0)
        
        train_loss /= len(train_loader.dataset)
        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.6f}')
        train_losses.append(train_loss)
        df = pd.DataFrame([[epoch + 1, train_loss]])
        df.to_csv('./result/CAO/loss/CAO_loss1.csv', mode='a', header=False, index=False)

    return train_losses








# Train the Model
num_epochs = 100
train_losses = train_model(num_epochs, model, train_normal_loader, criterion, optimizer)





# Plot Training Loss
plt.plot(train_losses, label='Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()





def plot_reconstructed_images(trainset_normal, model):
    n = 5
    fig = plt.figure(figsize=(10, 4))  # プロットのサイズを指定
    for i in range(n):
        # 元画像の表示
        plt.subplot(2, n, i + 1)
        img = trainset_normal[i][0].unsqueeze(0)
        plt.imshow(img.squeeze().numpy(), cmap='jet')
        plt.axis('off')  # 軸のラベルをオフにする
        
        # 再構成された画像の表示
        with torch.no_grad():
            img = img.float()
            output_1, output_2 = model(img)
            plt.subplot(2, n, i + 1 + n)
            output_2 = output_2.view(1, 80, 120)
            plt.imshow(output_2.squeeze().numpy(), cmap='jet')
            plt.axis('off')  # 軸のラベルをオフにする

    plt.tight_layout()  # レイアウトを自動調整
    plt.show()  # プロットを表示


plot_reconstructed_images(yxqDataset(data['X_training_normal'], 
                                     labels['X_training_normal'], 
                                     transform=data_tf), 
                          model
                         )





# Save the model
torch.save(model.state_dict(), './result/updated_model_CAO.pt')








# Load Model
model.load_state_dict(torch.load('./result/updated_model_CAO.pt'))





# Create Loaders
training_normal_loader = DataLoader(yxqDataset(data['X_training_normal'], labels['X_training_normal'], transform=data_tf), shuffle=False)
training_loader = DataLoader(yxqDataset(data['X_training'], labels['X_training'], transform=data_tf), shuffle=False)
test_loader = DataLoader(yxqDataset(data['X_test'], labels['X_test'], transform=data_tf), shuffle=False)
pre_loader = DataLoader(yxqDataset(data['X_pre'], labels['X_pre'], transform=data_tf), shuffle=False)





def CAE(loader, model, criterion):
    recon_list = []
    recon2_list = []
    loss_list = []
    labels_list = []
    model.eval()
    with torch.no_grad():
        for img, labels in loader:
            img = img.float()
            recon, recon2 = model(img)
            loss = criterion(recon2, img)
            loss_list.append(loss.item())
            recon_list.extend(recon.cpu().numpy())
            recon2_list.extend(recon2.cpu().numpy())
            labels_list.extend(labels.numpy())
    
    labels = np.array(labels_list)
    loss = np.array(loss_list)
    recon = np.array(recon_list)
    recon2_list = np.array(recon2_list).reshape(-1, 80, 120)
    return labels, loss, recon, recon2_list





# Compute Reconstruction Loss
training_normal_label, training_normal_loss, training_normal_recon, training_normal_recon2 = CAE(
    training_normal_loader, 
    model, 
    criterion
    )

training_label, training_loss, training_recon, training_recon2 = CAE(
    training_loader, 
    model, 
    criterion
    )





def box_outlier(df, label):
    Q1 = df.quantile(q=0.25)
    Q3 = df.quantile(q=0.75)
    up_whisker = Q3 + 1.5 * (Q3 - Q1)
    low_whisker = Q1 - 1.5 * (Q3 - Q1)
    
    outliers = df[(df > up_whisker) | (df < low_whisker)]
    data = pd.DataFrame({'id': outliers.index, 'Outlier': outliers, 'label': label[outliers.index]})
    return data, Q1, Q3, up_whisker, low_whisker, outliers.index





def OCC_CAE_class(label, loss, recon, up_whisker, training_normal_recon):
    loss_index_abnormal = np.flatnonzero(loss > up_whisker)
    boxplot_prelabel = np.zeros(len(label))
    boxplot_prelabel[loss_index_abnormal] = 1

    clf = PYOD_LOF(contamination=0.01)
    clf.fit(training_normal_recon)
    lof_prelabel = clf.predict(recon)

    oc_cae_prelabel = boxplot_prelabel + lof_prelabel
    oc_cae_prelabel[oc_cae_prelabel > 0] = 1
    
    return label, boxplot_prelabel, lof_prelabel, oc_cae_prelabel





def print_confusion_matrix(label, preds, set_name):
    print(f"{set_name} Classification:")
    print(f"Confusion Matrix:\n{metrics.confusion_matrix(label, preds)}")
    tp, fn, fp, tn = metrics.confusion_matrix(label, preds).ravel()
    print(f"tp: {tp}, fn: {fn}, fp: {fp}, tn: {tn}")
    specificity = tn / (tn + fp)
    print(f"Accuracy: {metrics.accuracy_score(label, preds)}")
    print(f"Specificity: {specificity}\n")





df_loss = pd.DataFrame(training_normal_loss, columns=['value'])
df_loss = df_loss.iloc[:, 0]


result, Q1, Q3, up_whisker, low_whisker, outlier_index = box_outlier(df_loss, training_normal_label)


# box plot
fig,axes = plt.subplots()
df_loss.plot(kind='box',ax=axes)


# Classify Training Set
label_training, Boxplot_training, LOF_training, OC_CAE_training = OCC_CAE_class(training_label, training_loss, training_recon, up_whisker, training_normal_recon)

print_confusion_matrix(label_training, Boxplot_training, "Training Set (Boxplot)")
print_confusion_matrix(label_training, LOF_training, "Training Set (LOF)")
print_confusion_matrix(label_training, OC_CAE_training, "Training Set (OC-CAE)")





# Reshape Data for Distance-Based Outlier Detection
def reshape_data(data):
    k = data.shape[2]
    data_reshaped = data.transpose(2, 0, 1).reshape(k, -1)
    return data_reshaped





X_training_normal_reshaped = reshape_data(data['X_training_normal'])
X_training_reshaped = reshape_data(data['X_training'])
X_test_reshaped = reshape_data(data['X_test'])
X_pre_reshaped = reshape_data(data['X_pre'])





# Distance-Based Outlier Detection
def dist(X):
    return np.sqrt(np.sum(X ** 2, axis=1))





dist_training_normal = dist(X_training_normal_reshaped)
dist_training = dist(X_training_reshaped)
dist_test = dist(X_test_reshaped)
dist_pre = dist(X_pre_reshaped)


df_dist = pd.DataFrame(dist_training_normal, columns=['value'])
df_dist = df_dist.iloc[:, 0]
result, Q1, Q3, up_whisker, low_whisker, outlier_index = box_outlier(df_dist, labels['X_training_normal'])





def dist_boxclass(dist, label, up_whisker, low_whisker):
    abnormal_indices = np.flatnonzero((dist < low_whisker) | (dist > up_whisker))
    dist_prelabel = np.zeros(len(label))
    dist_prelabel[abnormal_indices] = 1
    return dist_prelabel





dist_trainlabel = dist_boxclass(dist_training, labels['X_training'], up_whisker, low_whisker)
print_confusion_matrix(labels['X_training'], dist_trainlabel, "Training Set")

dist_testlabel = dist_boxclass(dist_test, labels['X_test'], up_whisker, low_whisker)
print_confusion_matrix(labels['X_test'], dist_testlabel, "Test Set")

dist_prelabel = dist_boxclass(dist_pre, labels['X_pre'], up_whisker, low_whisker)
print_confusion_matrix(labels['X_pre'], dist_prelabel, "Prediction Set")





# LOF Algorithm
clf_lof = PYOD_LOF(contamination=0.01)
clf_lof.fit(X_training_normal_reshaped)





#訓練セットの外れ値予測
lof_trainlabel = clf_lof.predict(X_training_reshaped)
print_confusion_matrix(labels['X_training'], lof_trainlabel, "Training Set (LOF)")

#テストセットの外れ値予測
lof_testlabel = clf_lof.predict(X_test_reshaped)
print_confusion_matrix(labels['X_test'], lof_testlabel, "Test Set (LOF)")

#予測セットの外れ値予測
lof_prelabel = clf_lof.predict(X_pre_reshaped)
print_confusion_matrix(labels['X_pre'], lof_prelabel, "Prediction Set (LOF)")





# One-Class SVM
gammas = np.logspace(-4, 2, 7)
nus = np.linspace(0.01, 0.99, 99)

best_score = 0
model_svm = OneClassSVM()

for gamma in gammas:
    for nu in nus:
        model_svm.set_params(kernel='rbf', gamma=gamma, nu=nu)
        model_svm.fit(X_training_normal_reshaped)
        y_train_pred = model_svm.predict(X_training_reshaped)
        y_test_pred = model_svm.predict(X_test_reshaped)
        y_pre_pred = model_svm.predict(X_pre_reshaped)

        y_train_pred = np.where(y_train_pred == 1, 0, 1)
        y_test_pred = np.where(y_test_pred == 1, 0, 1)
        y_pre_pred = np.where(y_pre_pred == 1, 0, 1)

        score = metrics.accuracy_score(labels['X_training'], y_train_pred) + \
                metrics.accuracy_score(labels['X_test'], y_test_pred) + \
                metrics.accuracy_score(labels['X_pre'], y_pre_pred)
        
        if score > best_score:
            best_score = score
            best_parameters = {'gamma': gamma, 'nu': nu}

print(f"Best Score: {best_score:.2f}")
print(f"Best Parameters: {best_parameters}")








model_svm.set_params(**best_parameters)
model_svm.fit(X_training_normal_reshaped)


def classify_with_svm(X_data, labels, model):
    preds = model.predict(X_data)
    preds = np.where(preds == 1, 0, 1)
    print_confusion_matrix(labels, preds, "Set")


classify_with_svm(X_training_reshaped, labels['X_training'], model_svm)
classify_with_svm(X_test_reshaped, labels['X_test'], model_svm)
classify_with_svm(X_pre_reshaped, labels['X_pre'], model_svm)






